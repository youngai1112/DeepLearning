{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BwTGaIZOIBO"
      },
      "source": [
        "### DeepLearning\n",
        "    - 사진 : 5000장 이상\n",
        "        - 데이터 부풀리기\n",
        "        - 이미지 센터링\n",
        "        - 모델의 변화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGnLFCAa1fy9"
      },
      "source": [
        "- 데이터 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "MnzfDxyFUWk6",
        "outputId": "ff837aa2-a5d7-4842-c537-ec9d0af46f0b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c95118cb-4688-4bac-9c87-7f65ce53c4a2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c95118cb-4688-4bac-9c87-7f65ce53c4a2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "up = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woUW7-NcUemW"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "with open('kaggle.json') as f:\n",
        "    kaggle = json.load(f)\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle['username']  # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = kaggle['key']            # key from the json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu3R3URcX8gi",
        "outputId": "3725f02b-dc58-452a-987f-ecf55c74cd37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading intel-image-classification.zip to /content\n",
            " 97% 335M/346M [00:02<00:00, 199MB/s]\n",
            "100% 346M/346M [00:03<00:00, 121MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d puneet6060/intel-image-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqaOcAHrYJS4"
      },
      "outputs": [],
      "source": [
        "!unzip intel-image-classification.zip > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6EsHwn81kSN"
      },
      "source": [
        "- 경로 / 항목 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNr_2UpmgSFy",
        "outputId": "550091df-b0a8-4d54-fac7-e94b308accfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "폴더: ['seg_train']\n",
            "경로명(dir_path):  seg_train\n",
            "이미지 파일의 갯수: 0\n",
            "--------------------------------------------------\n",
            "폴더: ['buildings', 'forest', 'sea', 'street', 'mountain', 'glacier']\n",
            "경로명(dir_path):  seg_train\n",
            "이미지 파일의 갯수: 0\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  buildings\n",
            "이미지 파일의 갯수: 2191\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  forest\n",
            "이미지 파일의 갯수: 2271\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  sea\n",
            "이미지 파일의 갯수: 2274\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  street\n",
            "이미지 파일의 갯수: 2382\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  mountain\n",
            "이미지 파일의 갯수: 2512\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  glacier\n",
            "이미지 파일의 갯수: 2404\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "# os.walk()를 사용하여 경로의 모든 하위 폴더와 파일을 탐색\n",
        "for dir_path, dir_names, filenames in os.walk('../content/seg_train'):\n",
        "    print(f'폴더: {dir_names}')\n",
        "    # os.path.basename(path): 경로 중 파일명만 얻기 \n",
        "    # print(f'{dir_path}'.split('/')[-1])\n",
        "    print('경로명(dir_path): ', os.path.basename(dir_path))     \n",
        "    print(f'이미지 파일의 갯수: {len(filenames)}')\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lwgIgdL8nqT",
        "outputId": "35cc46e6-40e4-4011-e57f-ebbdad077cfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "폴더: ['seg_test']\n",
            "경로명(dir_path):  seg_test\n",
            "이미지 파일의 갯수: 0\n",
            "--------------------------------------------------\n",
            "폴더: ['buildings', 'forest', 'sea', 'street', 'mountain', 'glacier']\n",
            "경로명(dir_path):  seg_test\n",
            "이미지 파일의 갯수: 0\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  buildings\n",
            "이미지 파일의 갯수: 437\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  forest\n",
            "이미지 파일의 갯수: 474\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  sea\n",
            "이미지 파일의 갯수: 510\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  street\n",
            "이미지 파일의 갯수: 501\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  mountain\n",
            "이미지 파일의 갯수: 525\n",
            "--------------------------------------------------\n",
            "폴더: []\n",
            "경로명(dir_path):  glacier\n",
            "이미지 파일의 갯수: 553\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "for dir_path, dir_names, filenames in os.walk('../content/seg_test'):\n",
        "    print(f'폴더: {dir_names}')\n",
        "    print('경로명(dir_path): ', os.path.basename(dir_path))     \n",
        "    print(f'이미지 파일의 갯수: {len(filenames)}')\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDFPljhygSDb"
      },
      "source": [
        "- 데이터 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWovY_7pDzXS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKO5mNfkgSAj"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1/225.,\n",
        "    rotation_range=10,          # 회전\n",
        "    shear_range=0.5,            # 이미지 찌그러트리기\n",
        "    width_shift_range=0.1,      # 좌우로 이동\n",
        "    height_shift_range=0.1,     # 상하로 이동\n",
        "    zoom_range=[0.5 ,2.0],      # 확대 또는 축소\n",
        "    horizontal_flip=True,       # 좌우 반전\n",
        "    vertical_flip=True,         # 상하 반전\n",
        "    fill_mode='nearest'         # 회전 축소 등으로 이미지에 여백이 생겼을 때 채우는 방법\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eXoS0hKeM6V",
        "outputId": "cd1a31ce-360e-43da-f49d-87fb1304066f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 150, 150, 3)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img1 = load_img('../content/seg_train/seg_train/buildings/0.jpg')\n",
        "x1 = img_to_array(img1)\n",
        "x1 = x1.reshape((1, ) + x1.shape)\n",
        "x1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AubxVxHj-FNA"
      },
      "outputs": [],
      "source": [
        "img2 = load_img('../content/seg_train/seg_train/forest/10007.jpg')\n",
        "x2 = img_to_array(img2)\n",
        "x2 = x2.reshape((1, ) + x2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJZeBaKo-GPQ"
      },
      "outputs": [],
      "source": [
        "img3 = load_img('../content/seg_train/seg_train/glacier/10.jpg')\n",
        "x3 = img_to_array(img3)\n",
        "x3 = x3.reshape((1, ) + x3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWNiEhLE-GzA"
      },
      "outputs": [],
      "source": [
        "img4 = load_img('../content/seg_train/seg_train/mountain/10000.jpg')\n",
        "x4 = img_to_array(img4)\n",
        "x4 = x4.reshape((1, ) + x4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ulz2P4Pl-HQ4"
      },
      "outputs": [],
      "source": [
        "img5 = load_img('../content/seg_train/seg_train/sea/1.jpg')\n",
        "x5 = img_to_array(img5)\n",
        "x5 = x5.reshape((1, ) + x5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgJQeblw-HnI"
      },
      "outputs": [],
      "source": [
        "img6 = load_img('../content/seg_train/seg_train/street/1000.jpg')\n",
        "x6 = img_to_array(img6)\n",
        "x6 = x6.reshape((1, ) + x6.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdgteqeR8Z5h"
      },
      "source": [
        "- 데이터 부풀리기를 통해 만들어지는 새로운 이미지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yJwLKfP7M0A"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f'preview'):\n",
        "    os.mkdir(f'preview')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96YlrhKM_SbY"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for _ in train_datagen.flow(x1, batch_size=1, save_to_dir='preview'):\n",
        "    count += 1\n",
        "    if count >= 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJmm1AUAEaO"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for _ in train_datagen.flow(x2, batch_size=1, save_to_dir='preview'):\n",
        "    count += 1\n",
        "    if count >= 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR_9nqJUAFHX"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for _ in train_datagen.flow(x3, batch_size=1, save_to_dir='preview'):\n",
        "    count += 1\n",
        "    if count >= 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srtt-j1AAFpH"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for _ in train_datagen.flow(x4, batch_size=1, save_to_dir='preview'):\n",
        "    count += 1\n",
        "    if count >= 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSAFwzP5AHTB"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for _ in train_datagen.flow(x5, batch_size=1, save_to_dir='preview'):\n",
        "    count += 1\n",
        "    if count >= 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1I0vezuAH3v"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for _ in train_datagen.flow(x6, batch_size=1, save_to_dir='preview'):\n",
        "    count += 1\n",
        "    if count >= 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohp2x3nKClsf"
      },
      "outputs": [],
      "source": [
        "target_size=(150,150)\n",
        "batch_size=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P064yUQeG4B6",
        "outputId": "7cca9327-9b35-4e8f-9021-2cf2ed8fc8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14034 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "train_data_generator = train_datagen.flow_from_directory(\n",
        "    'seg_train/seg_train',\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3XXoQgSC4R2",
        "outputId": "c14340e7-47dc-45fc-ac0d-6543214627a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3000 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_data_generator = test_datagen.flow_from_directory(\n",
        "    'seg_test/seg_test',\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaXto2QRDpH8",
        "outputId": "876951de-7309-45cc-bb7c-be1fd40b7a1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 5, 5, 5], dtype=int32)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_generator.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WJmWNNJCDsyN",
        "outputId": "8e2a1a8f-a2d0-459c-a0c9-5509666113b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'buildings/0.jpg'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_generator.filenames[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InTDNNFeDvfu"
      },
      "source": [
        "### 모델 정의/설정\n",
        "    - Conv2D\n",
        "    - MaxPooling2D\n",
        "    - Flatten\n",
        "    - Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc6UEOESDzmy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "seed = 2022\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8wN3iIWEGRJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRFGGyNaFtbo",
        "outputId": "3fb15fb8-5a2b-4248-b6ae-6526095b64e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18496)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               9470464   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,497,126\n",
            "Trainable params: 9,497,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model1 = Sequential([ \n",
        "    Conv2D(16, kernel_size=(3,3), input_shape=(150,150,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYGKmaosHS82"
      },
      "source": [
        "- 모델 설정\n",
        "    - class 갯수: 6개 => mountain, forest 또는 mountain, glacier의 이미지의 구분이 모호한 경우가 있음\n",
        "        - sparse_categorical_crossentropy는 각 샘플이 오직 하나의 class에 속할 때 사용\n",
        "        - categorical_crossentropy는 각 샘플이 여러개의 class에 속할 수 있을 때 사용 가능\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-nfQijxHXWf"
      },
      "outputs": [],
      "source": [
        "model1.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJZ-g9CfH3JW"
      },
      "source": [
        "- callback 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpVfYUupH-3-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "model_path = 'best-intel-v1.h5'\n",
        "cp = ModelCheckpoint(model_path, save_best_only=True, verbose=1)\n",
        "es = EarlyStopping(patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REL-3GxjIv52"
      },
      "source": [
        "- 모델 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvHhHiMgI6YU",
        "outputId": "6a38cd51-d85b-43bd-c635-7e9b4663594b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.3671 - accuracy: 0.4468\n",
            "Epoch 00001: val_loss improved from inf to 1.32538, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 34s 37ms/step - loss: 1.3671 - accuracy: 0.4468 - val_loss: 1.3254 - val_accuracy: 0.4700\n",
            "Epoch 2/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.2010 - accuracy: 0.5339\n",
            "Epoch 00002: val_loss improved from 1.32538 to 0.99962, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.2016 - accuracy: 0.5336 - val_loss: 0.9996 - val_accuracy: 0.6140\n",
            "Epoch 3/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.0870 - accuracy: 0.5719\n",
            "Epoch 00003: val_loss did not improve from 0.99962\n",
            "500/500 [==============================] - 19s 38ms/step - loss: 1.0860 - accuracy: 0.5724 - val_loss: 1.1508 - val_accuracy: 0.5870\n",
            "Epoch 4/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.0581 - accuracy: 0.5896\n",
            "Epoch 00004: val_loss improved from 0.99962 to 0.94437, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.0577 - accuracy: 0.5896 - val_loss: 0.9444 - val_accuracy: 0.6520\n",
            "Epoch 5/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.0021 - accuracy: 0.6032\n",
            "Epoch 00005: val_loss improved from 0.94437 to 0.92621, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.0021 - accuracy: 0.6032 - val_loss: 0.9262 - val_accuracy: 0.6450\n",
            "Epoch 6/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.0067 - accuracy: 0.5964\n",
            "Epoch 00006: val_loss did not improve from 0.92621\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.0066 - accuracy: 0.5964 - val_loss: 1.1236 - val_accuracy: 0.5830\n",
            "Epoch 7/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.9603 - accuracy: 0.6244\n",
            "Epoch 00007: val_loss improved from 0.92621 to 0.85085, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.9605 - accuracy: 0.6244 - val_loss: 0.8508 - val_accuracy: 0.6710\n",
            "Epoch 8/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.9418 - accuracy: 0.6445\n",
            "Epoch 00008: val_loss did not improve from 0.85085\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.9418 - accuracy: 0.6444 - val_loss: 0.9827 - val_accuracy: 0.6250\n",
            "Epoch 9/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.9175 - accuracy: 0.6533\n",
            "Epoch 00009: val_loss improved from 0.85085 to 0.77998, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.9180 - accuracy: 0.6528 - val_loss: 0.7800 - val_accuracy: 0.7060\n",
            "Epoch 10/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.8678 - accuracy: 0.6745\n",
            "Epoch 00010: val_loss did not improve from 0.77998\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.8702 - accuracy: 0.6740 - val_loss: 0.8312 - val_accuracy: 0.6790\n",
            "Epoch 11/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.8819 - accuracy: 0.6725\n",
            "Epoch 00011: val_loss did not improve from 0.77998\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.8847 - accuracy: 0.6716 - val_loss: 0.8229 - val_accuracy: 0.6770\n",
            "Epoch 12/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.8445 - accuracy: 0.6758\n",
            "Epoch 00012: val_loss improved from 0.77998 to 0.74791, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.8443 - accuracy: 0.6756 - val_loss: 0.7479 - val_accuracy: 0.7400\n",
            "Epoch 13/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.8258 - accuracy: 0.6886\n",
            "Epoch 00013: val_loss did not improve from 0.74791\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.8253 - accuracy: 0.6888 - val_loss: 0.7486 - val_accuracy: 0.7250\n",
            "Epoch 14/100\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.8643 - accuracy: 0.6724\n",
            "Epoch 00014: val_loss did not improve from 0.74791\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.8643 - accuracy: 0.6724 - val_loss: 0.9156 - val_accuracy: 0.6420\n",
            "Epoch 15/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.8346 - accuracy: 0.6886\n",
            "Epoch 00015: val_loss improved from 0.74791 to 0.71461, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.8341 - accuracy: 0.6892 - val_loss: 0.7146 - val_accuracy: 0.7370\n",
            "Epoch 16/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.8015 - accuracy: 0.6970\n",
            "Epoch 00016: val_loss did not improve from 0.71461\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 0.8022 - accuracy: 0.6964 - val_loss: 0.8479 - val_accuracy: 0.6750\n",
            "Epoch 17/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.7891 - accuracy: 0.6914\n",
            "Epoch 00017: val_loss improved from 0.71461 to 0.69723, saving model to best-intel-v1.h5\n",
            "500/500 [==============================] - 18s 37ms/step - loss: 0.7885 - accuracy: 0.6916 - val_loss: 0.6972 - val_accuracy: 0.7260\n",
            "Epoch 18/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.7701 - accuracy: 0.7150\n",
            "Epoch 00018: val_loss did not improve from 0.69723\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.7692 - accuracy: 0.7156 - val_loss: 0.7106 - val_accuracy: 0.7300\n",
            "Epoch 19/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7045\n",
            "Epoch 00019: val_loss did not improve from 0.69723\n",
            "500/500 [==============================] - 18s 36ms/step - loss: 0.7755 - accuracy: 0.7051 - val_loss: 0.8421 - val_accuracy: 0.6980\n",
            "Epoch 20/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.7811 - accuracy: 0.7102\n",
            "Epoch 00020: val_loss did not improve from 0.69723\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.7819 - accuracy: 0.7096 - val_loss: 0.7375 - val_accuracy: 0.7400\n",
            "Epoch 21/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.7775 - accuracy: 0.7166\n",
            "Epoch 00021: val_loss did not improve from 0.69723\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.7786 - accuracy: 0.7164 - val_loss: 0.7157 - val_accuracy: 0.7340\n",
            "Epoch 22/100\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.7221 - accuracy: 0.7407\n",
            "Epoch 00022: val_loss did not improve from 0.69723\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.7219 - accuracy: 0.7408 - val_loss: 0.7558 - val_accuracy: 0.7220\n"
          ]
        }
      ],
      "source": [
        "hist = model1.fit(\n",
        "    train_data_generator,\n",
        "    validation_data=test_data_generator,\n",
        "    epochs=100,\n",
        "    steps_per_epoch=500,\n",
        "    validation_steps=200,\n",
        "    verbose=1,\n",
        "    callbacks=[cp, es]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uMbgBwZKYmc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "best_model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MixgfXf1MQ3E"
      },
      "source": [
        "- 테스트 이미지 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUj0S_JOegjx",
        "outputId": "455d228c-751d-41ee-f3d0-68d98b9bccf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "600/600 [==============================] - 8s 14ms/step - loss: 0.7462 - accuracy: 0.7253\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.7461872696876526, 0.7253333330154419]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model1.evaluate(test_data_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG4HRFtzFUgO"
      },
      "source": [
        "### 모델 정의/설정\n",
        "    - Conv2D\n",
        "    - MaxPooling2D\n",
        "    - Flatten\n",
        "    - Dense\n",
        "    - Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYt6R6UQI3qE",
        "outputId": "a0864db6-999d-4805-becd-bfb51da8ad5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 148, 148, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 74, 74, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 74, 74, 16)        0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 36, 36, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 36, 36, 32)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 17, 17, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 17, 17, 64)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 18496)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               9470464   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,497,126\n",
            "Trainable params: 9,497,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model2 = Sequential([ \n",
        "    Conv2D(16, kernel_size=(3,3), input_shape=(150,150,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF6YqhR4FgzC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model2.compile(\n",
        "    optimizer=RMSprop(learning_rate=0.001, decay=1e-6),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plYvoL_uJ5AE"
      },
      "outputs": [],
      "source": [
        "model_path = 'best-intel-v2.h5'\n",
        "cp = ModelCheckpoint(model_path, save_best_only=True, verbose=1)\n",
        "es = EarlyStopping(patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZNHajlAKB7Y",
        "outputId": "8a3401bc-a168-4918-fd2f-2c08ecaef924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 1.4465 - accuracy: 0.4307\n",
            "Epoch 00001: val_loss improved from inf to 1.17818, saving model to best-intel-v2.h5\n",
            "1000/1000 [==============================] - 38s 37ms/step - loss: 1.4465 - accuracy: 0.4307 - val_loss: 1.1782 - val_accuracy: 0.5360\n",
            "Epoch 2/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.2012 - accuracy: 0.5301\n",
            "Epoch 00002: val_loss improved from 1.17818 to 1.13230, saving model to best-intel-v2.h5\n",
            "1000/1000 [==============================] - 34s 34ms/step - loss: 1.2006 - accuracy: 0.5306 - val_loss: 1.1323 - val_accuracy: 0.5500\n",
            "Epoch 3/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.1903 - accuracy: 0.5506\n",
            "Epoch 00003: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 34s 34ms/step - loss: 1.1898 - accuracy: 0.5508 - val_loss: 1.2040 - val_accuracy: 0.4940\n",
            "Epoch 4/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.1747 - accuracy: 0.5664\n",
            "Epoch 00004: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 34s 33ms/step - loss: 1.1756 - accuracy: 0.5658 - val_loss: 1.1594 - val_accuracy: 0.5950\n",
            "Epoch 5/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.2352 - accuracy: 0.5536\n",
            "Epoch 00005: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 33s 33ms/step - loss: 1.2353 - accuracy: 0.5534 - val_loss: 1.1758 - val_accuracy: 0.5440\n",
            "Epoch 6/300\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 1.2415 - accuracy: 0.5610\n",
            "Epoch 00006: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 35s 35ms/step - loss: 1.2415 - accuracy: 0.5610 - val_loss: 1.2221 - val_accuracy: 0.5470\n",
            "Epoch 7/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.2547 - accuracy: 0.5545\n",
            "Epoch 00007: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 35s 35ms/step - loss: 1.2543 - accuracy: 0.5543 - val_loss: 1.5324 - val_accuracy: 0.3270\n",
            "Epoch 8/300\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 1.2864 - accuracy: 0.5414\n",
            "Epoch 00008: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 35s 35ms/step - loss: 1.2864 - accuracy: 0.5414 - val_loss: 1.7620 - val_accuracy: 0.3440\n",
            "Epoch 9/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.2801 - accuracy: 0.5324\n",
            "Epoch 00009: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 35s 35ms/step - loss: 1.2800 - accuracy: 0.5325 - val_loss: 2.4294 - val_accuracy: 0.1700\n",
            "Epoch 10/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.4091 - accuracy: 0.5101\n",
            "Epoch 00010: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 34s 34ms/step - loss: 1.4091 - accuracy: 0.5098 - val_loss: 2.2507 - val_accuracy: 0.2150\n",
            "Epoch 11/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.3877 - accuracy: 0.5180\n",
            "Epoch 00011: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 35s 35ms/step - loss: 1.3874 - accuracy: 0.5179 - val_loss: 2.4073 - val_accuracy: 0.1690\n",
            "Epoch 12/300\n",
            " 999/1000 [============================>.] - ETA: 0s - loss: 1.3623 - accuracy: 0.5075\n",
            "Epoch 00012: val_loss did not improve from 1.13230\n",
            "1000/1000 [==============================] - 34s 34ms/step - loss: 1.3624 - accuracy: 0.5072 - val_loss: 2.4446 - val_accuracy: 0.1700\n"
          ]
        }
      ],
      "source": [
        "hist = model2.fit(\n",
        "    train_data_generator,\n",
        "    validation_data=test_data_generator,\n",
        "    epochs=300,\n",
        "    steps_per_epoch=1000,       \n",
        "    validation_steps=200,       \n",
        "    verbose=1,\n",
        "    callbacks=[cp, es]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW9MgQaPKXb8"
      },
      "outputs": [],
      "source": [
        "best_model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTh7SUKverUR",
        "outputId": "31f146c7-303b-4146-a0e6-568ae18042a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "600/600 [==============================] - 4s 7ms/step - loss: 2.4616 - accuracy: 0.1590\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[2.461552858352661, 0.1589999943971634]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.evaluate(test_data_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjERgByWXmH7"
      },
      "source": [
        "### 모델 정의/설정\n",
        "    - Conv2D\n",
        "    - MaxPooling2D\n",
        "    - Flatten\n",
        "    - Dense\n",
        "    - Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ-p-l6GZ9XW",
        "outputId": "fc03d292-2b9a-46dc-9f5c-6b0f0200b8f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 74, 74, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 36, 36, 64)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 34, 34, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 17, 17, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 17, 17, 64)        0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 18496)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 624)               11542128  \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 624)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 6)                 3750      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,602,198\n",
            "Trainable params: 11,602,198\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model3 = Sequential([ \n",
        "    Conv2D(32, kernel_size=(3,3), input_shape=(150,150,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(624, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8BQgPW5auKo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model3.compile(\n",
        "    optimizer=Adam(learning_rate=0.001, decay=1e-6),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KozJ8JLgbMYF"
      },
      "outputs": [],
      "source": [
        "model_path = 'best-intel-v3.h5'\n",
        "cp = ModelCheckpoint(model_path, save_best_only=True, verbose=1)\n",
        "es = EarlyStopping(patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H69rFi-ubQoR",
        "outputId": "c1ab5959-0314-4a5e-b5c1-b0b5b6a3ca62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.4987 - accuracy: 0.3629\n",
            "Epoch 00001: val_loss improved from inf to 1.68013, saving model to best-intel-v3.h5\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 1.4987 - accuracy: 0.3629 - val_loss: 1.6801 - val_accuracy: 0.3160\n",
            "Epoch 2/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.2645 - accuracy: 0.4938\n",
            "Epoch 00002: val_loss improved from 1.68013 to 1.14519, saving model to best-intel-v3.h5\n",
            "500/500 [==============================] - 18s 35ms/step - loss: 1.2637 - accuracy: 0.4940 - val_loss: 1.1452 - val_accuracy: 0.5100\n",
            "Epoch 3/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.1907 - accuracy: 0.5142\n",
            "Epoch 00003: val_loss did not improve from 1.14519\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 1.1898 - accuracy: 0.5148 - val_loss: 1.1914 - val_accuracy: 0.5160\n",
            "Epoch 4/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.1782 - accuracy: 0.5249\n",
            "Epoch 00004: val_loss improved from 1.14519 to 1.02464, saving model to best-intel-v3.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1787 - accuracy: 0.5246 - val_loss: 1.0246 - val_accuracy: 0.5720\n",
            "Epoch 5/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.1359 - accuracy: 0.5381\n",
            "Epoch 00005: val_loss did not improve from 1.02464\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.1360 - accuracy: 0.5382 - val_loss: 1.2842 - val_accuracy: 0.4720\n",
            "Epoch 6/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.1083 - accuracy: 0.5659\n",
            "Epoch 00006: val_loss improved from 1.02464 to 0.93108, saving model to best-intel-v3.h5\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 1.1072 - accuracy: 0.5664 - val_loss: 0.9311 - val_accuracy: 0.6220\n",
            "Epoch 7/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.0941 - accuracy: 0.5715\n",
            "Epoch 00007: val_loss did not improve from 0.93108\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.0932 - accuracy: 0.5720 - val_loss: 0.9567 - val_accuracy: 0.6080\n",
            "Epoch 8/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.0675 - accuracy: 0.5776\n",
            "Epoch 00008: val_loss did not improve from 0.93108\n",
            "500/500 [==============================] - 17s 33ms/step - loss: 1.0672 - accuracy: 0.5776 - val_loss: 0.9333 - val_accuracy: 0.6600\n",
            "Epoch 9/300\n",
            "500/500 [==============================] - ETA: 0s - loss: 1.0531 - accuracy: 0.5836\n",
            "Epoch 00009: val_loss did not improve from 0.93108\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.0531 - accuracy: 0.5836 - val_loss: 0.9772 - val_accuracy: 0.6140\n",
            "Epoch 10/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.0120 - accuracy: 0.6148\n",
            "Epoch 00010: val_loss improved from 0.93108 to 0.87423, saving model to best-intel-v3.h5\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.0118 - accuracy: 0.6148 - val_loss: 0.8742 - val_accuracy: 0.6540\n",
            "Epoch 11/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 1.0364 - accuracy: 0.5840\n",
            "Epoch 00011: val_loss did not improve from 0.87423\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 1.0371 - accuracy: 0.5836 - val_loss: 0.8935 - val_accuracy: 0.6700\n",
            "Epoch 12/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.9649 - accuracy: 0.6297\n",
            "Epoch 00012: val_loss did not improve from 0.87423\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.9657 - accuracy: 0.6296 - val_loss: 0.9226 - val_accuracy: 0.6740\n",
            "Epoch 13/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.9482 - accuracy: 0.6301\n",
            "Epoch 00013: val_loss did not improve from 0.87423\n",
            "500/500 [==============================] - 17s 35ms/step - loss: 0.9473 - accuracy: 0.6304 - val_loss: 1.1070 - val_accuracy: 0.6000\n",
            "Epoch 14/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.9942 - accuracy: 0.6168\n",
            "Epoch 00014: val_loss did not improve from 0.87423\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.9944 - accuracy: 0.6164 - val_loss: 0.9702 - val_accuracy: 0.6580\n",
            "Epoch 15/300\n",
            "499/500 [============================>.] - ETA: 0s - loss: 0.9571 - accuracy: 0.6188\n",
            "Epoch 00015: val_loss did not improve from 0.87423\n",
            "500/500 [==============================] - 17s 34ms/step - loss: 0.9569 - accuracy: 0.6188 - val_loss: 0.8826 - val_accuracy: 0.6480\n"
          ]
        }
      ],
      "source": [
        "hist = model3.fit(\n",
        "    train_data_generator,\n",
        "    validation_data=test_data_generator,\n",
        "    epochs=300,\n",
        "    steps_per_epoch=500,       \n",
        "    validation_steps=100,       \n",
        "    verbose=1,\n",
        "    callbacks=[cp, es]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM0pe-rfbWd6"
      },
      "outputs": [],
      "source": [
        "best_model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f8yx9iRew75",
        "outputId": "277d22e0-7e73-4031-a3e2-98f74d9630bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "600/600 [==============================] - 4s 7ms/step - loss: 0.9090 - accuracy: 0.6397\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.9089564085006714, 0.6396666765213013]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.evaluate(test_data_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjaBT6f4e0sI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "22-01-26_prj",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
